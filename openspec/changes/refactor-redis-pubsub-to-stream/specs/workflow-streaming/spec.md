# Specification: Workflow Streaming with Redis Stream

## ADDED Requirements

### Requirement: Redis Stream Event Persistence
The system SHALL persist all workflow node events to Redis Stream for durability and historical query support.

#### Scenario: Event published to Stream
- **WHEN** a workflow node completes execution
- **THEN** the event is added to Redis Stream with `XADD`
- **AND** the message ID is automatically generated by Redis
- **AND** the event includes: node_name, message_type, status, timestamp, data, execution_time_ms

#### Scenario: Stream auto-cleanup
- **WHEN** a Stream key accumulates more than 1000 messages
- **THEN** the system automatically trims the Stream using `XTRIM` with maxlen=1000
- **AND** the Stream key expires after 3600 seconds (1 hour)
- **AND** no manual cleanup is required

#### Scenario: Historical event recovery
- **WHEN** a client connects to the WebSocket endpoint
- **THEN** the system reads all historical events from the Stream using `XRANGE`
- **AND** sends all historical events to the client before subscribing to new events
- **AND** the client receives a complete event sequence from start to finish

---

### Requirement: Stream-based Subscription
The system SHALL support subscribing to workflow events via Redis Stream with blocking reads.

#### Scenario: Blocking read for new events
- **WHEN** a client has received all historical events
- **THEN** the system uses `XREAD` with block=1000 to wait for new events
- **AND** new events are pushed to the client as they arrive
- **AND** the system does not poll continuously (block timeout prevents busy-waiting)

#### Scenario: No message loss on client reconnection
- **WHEN** a client disconnects and reconnects
- **THEN** the system can resume from the last received message ID
- **AND** the client receives all missed events since disconnection
- **AND** no events are lost or duplicated

#### Scenario: Multiple concurrent clients
- **WHEN** multiple clients subscribe to the same workflow thread
- **THEN** each client receives all events independently
- **AND** events are not consumed or removed from the Stream
- **AND** each client can maintain its own last_id for resumption

---

### Requirement: Stream Key Naming and Organization
The system SHALL use a consistent naming scheme for Stream keys to organize workflow events.

#### Scenario: Single Stream per workflow
- **WHEN** a workflow is executed with a thread_id
- **THEN** all events for that workflow are stored in a single Stream key
- **AND** the key format is: `workflow:execution:{thread_id}`
- **AND** all node events (token, output, complete, error) are in the same Stream
- **AND** message ordering is guaranteed within the Stream

#### Scenario: Event field structure
- **WHEN** an event is added to the Stream
- **THEN** the event fields include:
  - `node_name`: Name of the LangGraph node (e.g., "query_or_respond", "tools")
  - `message_type`: Type of message (e.g., "token", "output", "complete", "error")
  - `status`: Current status (e.g., "streaming", "completed", "failed")
  - `timestamp`: Unix timestamp as string
  - `data`: JSON-encoded event data
  - `execution_time_ms`: Execution time in milliseconds (optional)

---

### Requirement: Backward Compatibility with Pub/Sub
The system SHALL support a transition period where both Redis Pub/Sub and Stream are active.

#### Scenario: Feature flag control
- **WHEN** REDIS_STREAM_ENABLED is set to false
- **THEN** the system uses Redis Pub/Sub for event publishing and subscription
- **AND** Stream functionality is disabled
- **AND** existing clients continue to work without changes

#### Scenario: Dual publishing
- **WHEN** REDIS_STREAM_ENABLED is set to true
- **THEN** events are published to both Redis Stream and Pub/Sub
- **AND** clients can choose to use either mechanism
- **AND** no events are lost during the transition

#### Scenario: Graceful degradation
- **WHEN** Redis Stream is not available (version < 5.0)
- **THEN** the system automatically falls back to Pub/Sub
- **AND** a warning is logged
- **AND** the application continues to function

---

### Requirement: Message Format Compatibility
The system SHALL maintain message format compatibility between Pub/Sub and Stream implementations.

#### Scenario: JSON serialization
- **WHEN** an event is published to Stream
- **THEN** the data field contains valid JSON
- **AND** LangChain BaseMessage objects are converted using message_to_dict()
- **AND** the message can be deserialized by clients expecting StreamMessage format

#### Scenario: Message ID tracking
- **WHEN** a client receives an event from Stream
- **THEN** the event includes a message_id field
- **AND** the client can use message_id for resumption (last_id parameter)
- **AND** the message_id format is compatible with Redis Stream IDs (e.g., "1705740000123-0")

---

### Requirement: Stream Resumption Support
The system SHALL support resuming event delivery from a specific point in the Stream.

#### Scenario: Resume from last_id
- **WHEN** a client provides a last_id parameter to the WebSocket endpoint
- **THEN** the system reads events from that point forward using `XREAD`
- **AND** the client receives only events after the specified ID
- **AND** no events are duplicated or skipped

#### Scenario: Resume after page refresh
- **WHEN** a user refreshes the browser page
- **THEN** the frontend can restore the last_id from localStorage or session
- **AND** the client reconnects with the saved last_id
- **AND** the user sees a continuous event stream without gaps

#### Scenario: Timeout handling
- **WHEN** a client has not received events for the block timeout period
- **THEN** the system returns from XREAD with empty results
- **AND** the client can retry or reconnect
- **AND** the connection remains healthy

---

## MODIFIED Requirements

### Requirement: Workflow Event Publishing
The system SHALL publish workflow node events for real-time progress tracking.

**Previous behavior**: Events published only to Redis Pub/Sub (non-persistent)

**New behavior**: Events published to Redis Stream (persistent) with optional Pub/Sub fallback

#### Scenario: Node output event
- **WHEN** a LangGraph node completes execution
- **THEN** the system publishes an event to Stream with:
  - node_name: the node identifier
  - message_type: "output"
  - status: "completed"
  - execution_time_ms: time taken in milliseconds
  - data: the node output state delta
- **AND** the event is persisted in Redis Stream
- **AND** the event can be queried later using XRANGE

#### Scenario: Token streaming event
- **WHEN** an LLM node generates tokens
- **THEN** the system publishes events to Stream with:
  - node_name: the LLM node identifier
  - message_type: "token"
  - status: "streaming"
  - data: the token content
- **AND** each token is a separate Stream entry
- **AND** tokens maintain order within the Stream

#### Scenario: Workflow completion event
- **WHEN** the entire workflow completes
- **THEN** the system publishes an event to Stream with:
  - node_name: "workflow"
  - message_type: "complete"
  - status: "completed"
  - data: node_times and total_ms statistics
- **AND** the event marks the end of the workflow
- **AND** clients can use this to stop listening for new events

#### Scenario: Workflow error event
- **WHEN** the workflow encounters an error or timeout
- **THEN** the system publishes an event to Stream with:
  - node_name: "workflow"
  - message_type: "error"
  - status: "failed"
  - data: error message and error_type
- **AND** the event is persisted for debugging
- **AND** clients receive the error immediately

---

### Requirement: WebSocket Event Subscription
The system SHALL support subscribing to workflow events via WebSocket with Stream-based delivery.

**Previous behavior**: WebSocket subscribed to Redis Pub/Sub channels (no history)

**New behavior**: WebSocket reads from Redis Stream (with history) and supports resumption

#### Scenario: WebSocket connection with history
- **WHEN** a client connects to `/ws/{thread_id}`
- **THEN** the system:
  1. Reads all historical events from Stream using XRANGE
  2. Sends all historical events to the client in order
  3. Records the last message ID
  4. Switches to XREAD for new events
- **AND** the client receives a complete event sequence
- **AND** no events are missed

#### Scenario: WebSocket resumption
- **WHEN** a client connects with `?last_id={message_id}`
- **THEN** the system:
  1. Skips historical read
  2. Uses XREAD from the specified last_id
  3. Sends only new events since that ID
- **AND** the client can resume without re-reading history
- **AND** the connection is faster for resumption

#### Scenario: WebSocket error handling
- **WHEN** the Stream read fails (e.g., network error)
- **THEN** the system:
  1. Logs the error
  2. Waits 1 second
  3. Retries the XREAD operation
- **AND** the client connection remains open
- **AND** the system eventually recovers

---

## REMOVED Requirements

### Requirement: Pub/Sub-only Event Delivery
**Reason**: Pub/Sub does not provide message persistence or resumption capability, leading to message loss on client disconnection or page refresh.

**Migration**: Use Redis Stream instead, which provides all Pub/Sub functionality plus persistence and resumption.

---

## Configuration Requirements

### Environment Variables
- `REDIS_STREAM_ENABLED` (boolean, default: false)
  - Controls whether to use Redis Stream or Pub/Sub
  - Supports gradual migration with feature flag

### Redis Requirements
- Minimum version: 5.0 (for Stream support)
- Recommended: 6.0+ (better performance and stability)
- Database: Can use any database (default: 2)

### Performance Targets
- Event publishing latency: < 10ms (same as Pub/Sub)
- Event delivery latency: < 50ms (same as Pub/Sub)
- Memory per 1000 events: ~500KB
- Stream cleanup overhead: < 1% CPU

---

## Acceptance Criteria

- ✅ All workflow events are persisted to Redis Stream
- ✅ Clients can recover history after page refresh
- ✅ Clients can resume from last_id after disconnection
- ✅ No events are lost or duplicated
- ✅ Performance is equivalent to Pub/Sub
- ✅ Backward compatibility with Pub/Sub is maintained
- ✅ Redis version < 5.0 gracefully falls back to Pub/Sub
- ✅ All unit and integration tests pass
