"""èŠå¤©ç›¸å…³çš„ API ç«¯ç‚¹ã€‚"""

from __future__ import annotations

import asyncio
import logging
import time
from typing import Any

from fastapi import APIRouter, BackgroundTasks, Depends, HTTPException, status

from api.dependencies import get_graph, get_redis_publisher
from db.checkpointer import CheckpointerManager
from api.schemas import (
    ChatRequest,
    ChatResponse,
    StreamStartResponse,
    HistoryMessage,
    ThreadHistory,
    TraceRun,
    ThreadHistoryWithTrace,
)
from api.utils import extract_content
from langchain_core.messages import AIMessage, BaseMessage
from config.settings import get_settings
from infra.redis_pubsub import RedisPublisher, StreamMessage
from utils.langsmith_client import get_langsmith_client

logger = logging.getLogger(__name__)

try:
    from langchain_core.messages import BaseMessage, message_to_dict  # type: ignore
except Exception:  # pragma: no cover - é˜²å¾¡æ€§å¤„ç†
    BaseMessage = None  # type: ignore[assignment]
    message_to_dict = None  # type: ignore[assignment]


router = APIRouter()


def build_trace_tree(trace_runs: list[TraceRun]) -> list[dict[str, Any]]:
    """æ„å»º trace æ‰§è¡Œæ ‘ã€‚
    
    Args:
        trace_runs: TraceRun å¯¹è±¡åˆ—è¡¨
        
    Returns:
        æ ‘å½¢ç»“æ„çš„ trace runsï¼ˆåªåŒ…å«æ ¹èŠ‚ç‚¹ï¼Œå­èŠ‚ç‚¹åœ¨ children å­—æ®µä¸­ï¼‰
    """
    if not trace_runs:
        return []
    
    # å°† Pydantic å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
    runs_dict = [r.model_dump() for r in trace_runs]
    
    # å»ºç«‹ç´¢å¼•
    run_map = {r["run_id"]: r for r in runs_dict}
    children_map = {r["run_id"]: [] for r in runs_dict}
    
    # æ‰¾åˆ°æ ¹èŠ‚ç‚¹å’Œå­èŠ‚ç‚¹å…³ç³»
    roots = []
    for r in runs_dict:
        parent_id = r.get("parent_run_id")
        if parent_id and parent_id in children_map:
            children_map[parent_id].append(r)
        else:
            roots.append(r)
    
    # æ·±åº¦ä¼˜å…ˆæœç´¢æ„å»ºæ ‘èŠ‚ç‚¹
    def build_node(run: dict[str, Any]) -> dict[str, Any]:
        children = children_map.get(run["run_id"], [])
        node = {
            "run_id": run["run_id"],
            "name": run["name"],
            "run_type": run["run_type"],
            "start_time": run["start_time"],
            "end_time": run.get("end_time"),
            "latency_ms": run.get("latency_ms"),
            "total_tokens": run.get("total_tokens"),
            "prompt_tokens": run.get("prompt_tokens"),
            "completion_tokens": run.get("completion_tokens"),
            "error": run.get("error"),
            "inputs": run.get("inputs"),
            "outputs": run.get("outputs"),
            "parent_run_id": run.get("parent_run_id"),
            "children": [build_node(c) for c in children]
        }
        return node
    
    return [build_node(r) for r in roots]


def _normalize_update(obj: Any) -> Any:
    """é€’å½’è½¬æ¢ LangChain æ¶ˆæ¯å¯¹è±¡ä¸º JSON å¯åºåˆ—åŒ–æ ¼å¼ã€‚

    âœ… å¤„ç†æµç¨‹ï¼š
    1. å¦‚æœæ˜¯ BaseMessageï¼ˆå¦‚ AIMessageï¼‰ï¼Œä½¿ç”¨ message_to_dict è½¬æ¢
    2. å¦‚æœæ˜¯å­—å…¸/åˆ—è¡¨/å…ƒç»„ï¼Œé€’å½’å¤„ç†æ‰€æœ‰åµŒå¥—å…ƒç´ 
    3. å…¶ä»–ç±»å‹ç›´æ¥è¿”å›

    å‚æ•°ï¼š
    - obj: å¾…è½¬æ¢çš„å¯¹è±¡ï¼ˆå¯èƒ½æ˜¯ BaseMessageã€dictã€listã€tuple ç­‰ï¼‰

    è¿”å›ï¼š
    - è½¬æ¢åçš„ JSON å‹å¥½æ ¼å¼å¯¹è±¡

    æ³¨æ„ï¼š
    - æ­¤å‡½æ•°ç”¨äºåœ¨å‘å¸ƒåˆ° Redis å‰é¢„å¤„ç† LangGraph çš„ state delta
    - ç¡®ä¿ AIMessage ç­‰ LangChain å¯¹è±¡èƒ½è¢«æ­£ç¡®åºåˆ—åŒ–
    """
    if BaseMessage is not None and isinstance(obj, BaseMessage):
        if message_to_dict is not None:
            return message_to_dict(obj)
        return {"type": obj.__class__.__name__, "content": getattr(obj, "content", "")}
    if isinstance(obj, dict):
        return {k: _normalize_update(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [_normalize_update(v) for v in obj]
    if isinstance(obj, tuple):
        return tuple(_normalize_update(v) for v in obj)
    return obj


async def _stream_workflow_to_redis(
    *,
    graph,
    payload: dict[str, Any],
    config: dict[str, Any],
    thread_id: str,
    publisher: RedisPublisher,
) -> None:
    """åå°æ‰§è¡Œå·¥ä½œæµå¹¶å°†èŠ‚ç‚¹æ›´æ–°å‘å¸ƒåˆ° Redisã€‚

    âœ… ç®€åŒ–æµç¨‹ï¼š
    1. ä½¿ç”¨ stream_mode=["updates", "messages", "custom"] æ··åˆæ¨¡å¼
    2. åªå‘å¸ƒå…³é”®äº‹ä»¶ï¼ˆcompletedã€tokenã€customï¼‰
    3. ç§»é™¤ start äº‹ä»¶ï¼Œä¿æŒç•Œé¢æ•´æ´
    4. å·¥ä½œæµå®Œæˆåå‘å¸ƒ workflow:complete
    5. é”™è¯¯æ—¶å‘å¸ƒ workflow:error

    å‚æ•°ï¼š
    - graph: LangGraph å·¥ä½œæµå®ä¾‹
    - payload: è¾“å…¥æ•°æ®ï¼ŒåŒ…å« messages æ•°ç»„
    - config: LangGraph é…ç½®ï¼ŒåŒ…å« thread_idã€user_id
    - thread_id: ä¼šè¯çº¿ç¨‹ IDï¼Œç”¨äº Redis é¢‘é“å‘½å
    - publisher: Redis å‘å¸ƒå™¨å®ä¾‹

    äº‹ä»¶å‘å¸ƒï¼š
    - workflow:{thread_id}:{node_name}:token - LLM token æµå¼è¾“å‡º
    - workflow:{thread_id}:{node_name}:output - èŠ‚ç‚¹å®Œæˆ
    - workflow:{thread_id}:custom:custom - è‡ªå®šä¹‰çŠ¶æ€æ›´æ–°
    - workflow:{thread_id}:workflow:complete - å·¥ä½œæµå®Œæˆ
    - workflow:{thread_id}:workflow:error - å·¥ä½œæµé”™è¯¯

    æ³¨æ„ï¼š
    - æ··åˆæ¨¡å¼è¿”å› (mode, chunk) å…ƒç»„
    - messages æ¨¡å¼ï¼šLLM token æµå¼è¾“å‡º
    - updates æ¨¡å¼ï¼šèŠ‚ç‚¹å®Œæˆäº‹ä»¶ï¼ˆæ—  start äº‹ä»¶ï¼‰
    - custom æ¨¡å¼ï¼šè‡ªå®šä¹‰è¿›åº¦æç¤ºï¼ˆä»èŠ‚ç‚¹å†…éƒ¨å‘é€ï¼‰
    - è¶…æ—¶æ—¶é—´ç”± WORKFLOW_TIMEOUT_SECONDS é…ç½®ï¼ˆé»˜è®¤ 300 ç§’ï¼‰
    """
    start_time = time.perf_counter()
    node_times: dict[str, float] = {}

    settings = get_settings()
    timeout_seconds = settings.workflow_timeout_seconds

    async def _process_stream():
        async for stream_mode, chunk in graph.astream(
            payload,
            config,
            stream_mode=["updates", "messages", "custom"],
        ):
            if stream_mode == "messages":
                # messages æ¨¡å¼ï¼šchunk æ˜¯ (message_chunk, metadata) å…ƒç»„
                # ç”¨äºæµå¼è¾“å‡º LLM ç”Ÿæˆçš„ token
                message_chunk, metadata = chunk
                node_name = metadata.get("langgraph_node", "unknown")
                
                # ä»…å¤„ç† LLM èŠ‚ç‚¹çš„ tokenï¼ˆquery_or_respond å’Œ generateï¼‰
                if node_name in ("query_or_respond", "generate"):
                    # æå– token å†…å®¹
                    token_content = ""
                    if hasattr(message_chunk, "content"):
                        token_content = str(message_chunk.content) if message_chunk.content else ""
                    elif hasattr(message_chunk, "text"):
                        token_content = str(message_chunk.text) if message_chunk.text else ""
                    else:
                        # è§„èŒƒåŒ–åä»å­—å…¸æå–
                        normalized_chunk = _normalize_update(message_chunk)
                        if isinstance(normalized_chunk, dict):
                            if "content" in normalized_chunk:
                                token_content = str(normalized_chunk["content"])
                            elif isinstance(normalized_chunk.get("text"), str):
                                token_content = normalized_chunk["text"]
                    
                    # ä»…å‘é€éç©ºçš„ token
                    if token_content:
                        await publisher.publish_node_output(
                            thread_id=thread_id,
                            node_name=node_name,
                            data={
                                "token": token_content,
                                "chunk": _normalize_update(message_chunk),
                                "metadata": _normalize_update(metadata),
                            },
                            status="streaming",
                            message_type="token",
                        )
                
            elif stream_mode == "updates":
                # updates æ¨¡å¼ï¼šèŠ‚ç‚¹å®Œæˆäº‹ä»¶ï¼ˆç§»é™¤ start äº‹ä»¶ï¼‰
                for node_name, update in chunk.items():
                    elapsed_ms = (time.perf_counter() - start_time) * 1000
                    node_times[node_name] = elapsed_ms

                    normalized = _normalize_update(update)
                    # ä»…å‘å¸ƒèŠ‚ç‚¹å®Œæˆäº‹ä»¶
                    await publisher.publish_node_output(
                        thread_id=thread_id,
                        node_name=node_name,
                        data=normalized,
                        status="completed",
                        message_type="output",
                        execution_time_ms=elapsed_ms,
                    )
            
            elif stream_mode == "custom":
                # custom æ¨¡å¼ï¼šè‡ªå®šä¹‰çŠ¶æ€æ›´æ–°ï¼ˆä»èŠ‚ç‚¹å†…éƒ¨å‘é€ï¼‰
                await publisher.publish_message(
                    StreamMessage(
                        thread_id=thread_id,
                        node_name="custom",
                        message_type="custom",
                        status="info",
                        timestamp=time.time(),
                        data=chunk,
                    )
                )

    try:
        await asyncio.wait_for(_process_stream(), timeout=timeout_seconds)

        total_ms = (time.perf_counter() - start_time) * 1000
        await publisher.publish_workflow_complete(
            thread_id=thread_id,
            data={"node_times": node_times, "total_ms": total_ms},
            execution_time_ms=total_ms,
        )
    except asyncio.CancelledError:
        logger.info(f"Workflow cancelled: {thread_id}")
    except asyncio.TimeoutError:
        logger.warning(f"Workflow timeout after {timeout_seconds}s: {thread_id}")
        await publisher.publish_workflow_error(
            thread_id=thread_id,
            error=f"Workflow execution exceeded {timeout_seconds}s timeout",
            data={"error_type": "timeout", "timeout_seconds": timeout_seconds},
        )
    except Exception as exc:  # noqa: BLE001
        logger.exception(f"Workflow error: {exc}")
        await publisher.publish_workflow_error(
            thread_id=thread_id,
            error=str(exc),
            data={"error_type": "execution_error"},
        )


@router.post("/stream", response_model=StreamStartResponse)
async def chat_stream_endpoint(
    req: ChatRequest,
    background_tasks: BackgroundTasks,
    graph=Depends(get_graph),
    publisher: RedisPublisher = Depends(get_redis_publisher),
):
    """å¯åŠ¨æµå¼èŠå¤©å·¥ä½œæµå¹¶å‘å¸ƒèŠ‚ç‚¹æ›´æ–°åˆ° Redisã€‚

    âœ… æµç¨‹ï¼š
    1. ç«‹å³è¿”å› thread_id å’Œ WebSocket è®¢é˜…é¢‘é“
    2. åå°å¯åŠ¨ graph.astreamï¼Œå‘é€èŠ‚ç‚¹äº‹ä»¶åˆ° Redis
    3. å‰ç«¯é€šè¿‡ WebSocket è®¢é˜… Redis é¢‘é“æ¥æ”¶å®æ—¶æ›´æ–°

    è¿”å›çš„å“åº”åŒ…å«ï¼š
    - thread_id: å·¥ä½œæµä¼šè¯æ ‡è¯†ï¼Œç”¨äºåç»­çŠ¶æ€æŸ¥è¯¢å’Œæ¢å¤
    - user_id: ç”¨æˆ·æ ‡è¯†ï¼ˆå¯é€‰ï¼‰ï¼Œç”¨äºè®°å¿†å‘½åç©ºé—´
    - ws_channel: Redis é¢‘é“æ¨¡å¼ï¼Œæ ¼å¼ä¸º workflow:{thread_id}:*
    - status: å›ºå®šä¸º "streaming"ï¼Œè¡¨ç¤ºæµå¼æ‰§è¡Œå·²å¯åŠ¨

    å‚æ•°ï¼š
    - req: èŠå¤©è¯·æ±‚ï¼ŒåŒ…å« thread_idã€user_idã€message
    - background_tasks: FastAPI åå°ä»»åŠ¡ç®¡ç†å™¨
    - graph: LangGraph å·¥ä½œæµå®ä¾‹ï¼ˆé€šè¿‡ä¾èµ–æ³¨å…¥ï¼‰
    - publisher: Redis å‘å¸ƒå™¨å®ä¾‹ï¼ˆé€šè¿‡ä¾èµ–æ³¨å…¥ï¼‰

    è¿”å›ï¼š
    - StreamStartResponse: åŒ…å« thread_idã€ws_channelã€status

    å‰ç«¯ä½¿ç”¨æ–¹å¼ï¼š
    1. è°ƒç”¨æ­¤æ¥å£è·å– thread_id å’Œ ws_channel
    2. è¿æ¥ WebSocket: ws://host/ws/{thread_id}
    3. è®¢é˜… Redis é¢‘é“: workflow:{thread_id}:*
    4. æ¥æ”¶å®æ—¶èŠ‚ç‚¹æ›´æ–°äº‹ä»¶

    æ³¨æ„ï¼š
    - æ­¤æ¥å£ç«‹å³è¿”å›ï¼Œä¸ç­‰å¾…å·¥ä½œæµå®Œæˆ
    - å®é™…å·¥ä½œæµåœ¨åå°å¼‚æ­¥æ‰§è¡Œï¼Œé€šè¿‡ Redis Pub/Sub æ¨é€æ›´æ–°
    - å‰ç«¯éœ€è¦ä¸»åŠ¨è®¢é˜… WebSocket æ‰èƒ½æ¥æ”¶æ›´æ–°
    - åŒä¸€ä¸ª thread_id å¯ä»¥å¤šæ¬¡è°ƒç”¨ï¼Œå…±äº«å¯¹è¯ä¸Šä¸‹æ–‡
    """
    config: dict[str, Any] = {"configurable": {"thread_id": req.thread_id}}
    if req.user_id:
        config["configurable"]["user_id"] = req.user_id
    if req.chat_model:
        config["configurable"]["chat_model"] = req.chat_model
    if req.enable_websearch:
        config["configurable"]["enable_websearch"] = req.enable_websearch

    # Combine message with uploaded documents for LLM
    message_content = req.message
    if req.documents:
        # Add document metadata markers for frontend extraction
        doc_section = "\n\n<uploaded_documents>\n"
        for idx, doc in enumerate(req.documents):
            # Include metadata in markers for frontend to parse
            doc_section += f'<document index="{idx}" filename="{doc.filename}" format="{doc.format}">\n{doc.markdown_content}\n</document>\n'
        doc_section += "</uploaded_documents>"
        message_content += doc_section

    payload: dict[str, Any] = {
        "messages": [{"role": "user", "content": message_content}]
    }

    background_tasks.add_task(
        _stream_workflow_to_redis,
        graph=graph,
        payload=payload,
        config=config,
        thread_id=req.thread_id,
        publisher=publisher,
    )

    return StreamStartResponse(
        thread_id=req.thread_id,
        user_id=req.user_id,
        ws_channel=f"workflow:{req.thread_id}:*",
        status="streaming",
    )


def _filter_user_visible_messages(messages: list[BaseMessage], thread_id: str) -> list[HistoryMessage]:
    """è¿‡æ»¤å¹¶è½¬æ¢æ¶ˆæ¯ä¸ºç”¨æˆ·å¯è§çš„æ ¼å¼ã€‚
    
    åªä¿ç•™æœ€ç»ˆæ¶ˆæ¯ï¼ˆç”¨æˆ·è¾“å…¥å’Œæœ€ç»ˆ LLM è¾“å‡ºï¼‰ï¼Œæ’é™¤ä¸­é—´èŠ‚ç‚¹çš„è¾“å‡ºã€‚
    
    è¿‡æ»¤è§„åˆ™ï¼š
    - HumanMessage: ä¿ç•™æ‰€æœ‰
    - AIMessage: åªä¿ç•™æ²¡æœ‰ tool_calls çš„ï¼ˆæœ€ç»ˆå›å¤ï¼‰
    - ToolMessage: è·³è¿‡ï¼ˆä¸­é—´è¿‡ç¨‹ï¼‰
    - SystemMessage: è·³è¿‡ï¼ˆç³»ç»Ÿå†…éƒ¨ï¼‰
    """
    from langchain_core.messages import HumanMessage, ToolMessage, SystemMessage
    
    history_messages: list[HistoryMessage] = []
    
    for i, msg in enumerate(messages):
        # è·³è¿‡ä¸­é—´æ¶ˆæ¯ç±»å‹
        if isinstance(msg, (ToolMessage, SystemMessage)):
            continue
        
        # AI æ¶ˆæ¯ï¼šè·³è¿‡å¸¦ tool_calls çš„ï¼ˆä¸­é—´å†³ç­–è¿‡ç¨‹ï¼‰
        if isinstance(msg, AIMessage):
            if msg.tool_calls:
                continue
            # è·³è¿‡ç©ºå†…å®¹çš„ AI æ¶ˆæ¯
            if not msg.content:
                continue
        
        # æå–åŸºç¡€å­—æ®µ
        msg_type = getattr(msg, "type", "unknown")
        role = "user" if isinstance(msg, HumanMessage) else "assistant"
        content = extract_content(msg)
        msg_id = getattr(msg, "id", None) or f"{thread_id}_{i}"
        
        # æå–æ—¶é—´æˆ³
        timestamp = None
        if hasattr(msg, "timestamp"):
            timestamp = msg.timestamp
        elif hasattr(msg, "response_metadata"):
            metadata = getattr(msg, "response_metadata", {})
            if isinstance(metadata, dict):
                timestamp = metadata.get("timestamp")
        
        history_messages.append(
            HistoryMessage(
                id=str(msg_id),
                role=role,
                content=str(content) if content else "",
                timestamp=timestamp,
                type=msg_type,
                name=getattr(msg, "name", None),
                tool_calls=getattr(msg, "tool_calls", []),
                tool_call_id=getattr(msg, "tool_call_id", None),
                artifact=getattr(msg, "artifact", None),
            )
        )
    
    return history_messages


@router.get("/threads/{thread_id}/history", response_model=ThreadHistory)
async def get_thread_history(
    thread_id: str,
    graph=Depends(get_graph),
):
    """è·å–çº¿ç¨‹çš„å¯¹è¯å†å²ï¼ˆè½»é‡çº§ï¼Œä¸å« Traceï¼‰ã€‚

    âœ… ç”¨é€”ï¼š
    - å‰ç«¯é»˜è®¤ä½¿ç”¨æ­¤æ¥å£åŠ è½½å†å²å¯¹è¯
    - åªè¿”å›ç”¨æˆ·è¾“å…¥å’Œæœ€ç»ˆ LLM è¾“å‡ºï¼Œæ’é™¤ä¸­é—´èŠ‚ç‚¹çš„å¤„ç†è¿‡ç¨‹
    - æ€§èƒ½ä¼˜åŒ–ï¼Œä¸æŸ¥è¯¢ LangSmith Trace API
    - é€‚ç”¨äºå¯¹è¯æ¢å¤ã€å†å²æµè§ˆç­‰åœºæ™¯

    âœ… æµç¨‹ï¼š
    1. ä» LangGraph Checkpoint è·å–å½“å‰çŠ¶æ€
    2. è¿‡æ»¤æ¶ˆæ¯ï¼šåªä¿ç•™ç”¨æˆ·æ¶ˆæ¯å’Œæœ€ç»ˆ AI å›å¤
    3. è½¬æ¢ä¸º HistoryMessage æ ¼å¼è¿”å›

    å‚æ•°ï¼š
    - thread_id: ä¼šè¯çº¿ç¨‹æ ‡è¯†

    è¿”å›ï¼š
    - ThreadHistory: åŒ…å« thread_idã€messages åˆ—è¡¨ã€total_messages

    æ³¨æ„ï¼š
    - å¦‚æœçº¿ç¨‹ä¸å­˜åœ¨ï¼Œè¿”å› 404 é”™è¯¯
    - è‡ªåŠ¨è¿‡æ»¤æ‰ä¸­é—´èŠ‚ç‚¹çš„è¾“å‡ºï¼ˆå¦‚ query_or_respond çš„ tool_callsã€tools èŠ‚ç‚¹çš„è¾“å‡ºï¼‰
    """
    try:
        config = {"configurable": {"thread_id": thread_id}}
        state = await graph.aget_state(config)
        
        if not state:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"Thread not found: {thread_id}",
            )

        messages = state.values.get("messages", [])
        history_messages = _filter_user_visible_messages(messages, thread_id)

        return ThreadHistory(
            thread_id=thread_id,
            messages=history_messages,
            total_messages=len(history_messages),
        )

    except HTTPException:
        raise
    except Exception as exc:
        logger.exception(f"Error fetching thread history: {exc}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch thread history: {str(exc)}",
        ) from exc


@router.get("/threads/{thread_id}/history-with-trace", response_model=ThreadHistoryWithTrace)
async def get_thread_history_with_trace(thread_id: str, graph=Depends(get_graph)):
    """è·å–çº¿ç¨‹çš„å®Œæ•´å†å²è®°å½•ï¼ˆå« LangSmith Trace ç»Ÿè®¡ï¼‰ã€‚

    âœ… ç”¨é€”ï¼š
    - ç”¨äºè°ƒè¯•ã€æ€§èƒ½åˆ†æã€Token ç»Ÿè®¡ç­‰åœºæ™¯
    - è¿”å›å®Œæ•´çš„ LangSmith Trace æ ‘,åŒ…å«æ‰€æœ‰èŠ‚ç‚¹æ‰§è¡Œä¿¡æ¯
    - æä¾›æ ¹èŠ‚ç‚¹ç»Ÿè®¡(æ€»å»¶è¿Ÿã€æ€» Token æ¶ˆè€—ç­‰)
    - å‰ç«¯å¯é€‰æ‹©æ€§ä½¿ç”¨æ­¤æ¥å£è·å–è¯¦ç»†æ‰§è¡Œä¿¡æ¯

    âœ… æµç¨‹ï¼š
    1. ä» LangGraph Checkpoint è·å–åŸºæœ¬æ¶ˆæ¯å†å²
    2. ä» LangSmith API æŸ¥è¯¢è¯¥ thread çš„æ‰€æœ‰ Trace Runs
    3. æ„å»º Trace æ ‘å½¢ç»“æ„(çˆ¶å­å…³ç³»)
    4. é€’å½’ç»Ÿè®¡æ‰€æœ‰èŠ‚ç‚¹çš„ Token æ¶ˆè€—
    5. åˆå¹¶è¿”å›å®Œæ•´æ•°æ®(æ¶ˆæ¯ + Trace æ ‘ + ç»Ÿè®¡ä¿¡æ¯)

    å‚æ•°ï¼š
    - thread_id: ä¼šè¯çº¿ç¨‹æ ‡è¯†

    è¿”å›ï¼š
    - ThreadHistoryWithTrace: åŒ…å«æ¶ˆæ¯ã€trace_runsã€trace_treeã€ç»Ÿè®¡ä¿¡æ¯

    æ³¨æ„ï¼š
    - å¦‚æœæœªé…ç½® LangSmith,trace_runs å°†ä¸ºç©ºæ•°ç»„
    - trace_runs æŒ‰ start_time æ­£åºæ’åˆ—(ä»æ—©åˆ°æ™š)
    - æŸ¥è¯¢ LangSmith API æœ‰ç½‘ç»œå¼€é”€,å»ºè®®æŒ‰éœ€ä½¿ç”¨
    - å‰ç«¯é»˜è®¤ä¸ä½¿ç”¨æ­¤æ¥å£,é¿å…æ€§èƒ½å½±å“
    """
    try:
        config = {"configurable": {"thread_id": thread_id}}

        # 1. è·å–åŸºæœ¬æ¶ˆæ¯å†å²
        state = await graph.aget_state(config)
        if not state:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"çº¿ç¨‹æœªæ‰¾åˆ°ï¼š{thread_id}",
            )

        messages = state.values.get("messages", [])
        history_messages = _filter_user_visible_messages(messages, thread_id)

        # 2. ä» LangSmith è·å– Trace Runs
        trace_runs: list[TraceRun] = []
        langsmith_client = get_langsmith_client()
        
        if langsmith_client:
            try:
                settings = get_settings()
                # ä½¿ç”¨ LangSmith å®˜æ–¹æ¨èçš„ filter è¯­æ³•æŸ¥è¯¢ thread_id
                filter_string = (
                    f'and(in(metadata_key, ["session_id","conversation_id","thread_id"]), '
                    f'eq(metadata_value, "{thread_id}"))'
                )
                
                runs = []
                for run in langsmith_client.list_runs(
                    project_name=settings.langsmith_project,
                    filter=filter_string,
                ):
                    runs.append(run)
                
                # æŒ‰ start_time å‡åºæ’åºï¼ˆä»æ—©åˆ°æ™šï¼‰
                runs.sort(key=lambda r: r.start_time if r.start_time else 0)
                
                # è½¬æ¢ä¸º TraceRun
                for run in runs:
                    latency_ms = None
                    if run.end_time and run.start_time:
                        latency_ms = (run.end_time - run.start_time).total_seconds() * 1000
                    
                    trace_run = TraceRun(
                        run_id=str(run.id),
                        name=run.name,
                        run_type=run.run_type,
                        start_time=run.start_time.isoformat() if run.start_time else "",
                        end_time=run.end_time.isoformat() if run.end_time else None,
                        latency_ms=latency_ms,
                        total_tokens=run.total_tokens,
                        prompt_tokens=run.prompt_tokens,
                        completion_tokens=run.completion_tokens,
                        error=run.error,
                        inputs=run.inputs,
                        outputs=run.outputs,
                        parent_run_id=str(run.parent_run_id) if run.parent_run_id else None,
                    )
                    trace_runs.append(trace_run)
                
                logger.info(f"ä» LangSmith è·å–äº† {len(trace_runs)} ä¸ª trace runsï¼Œçº¿ç¨‹ {thread_id}")
            except Exception as exc:
                logger.warning(f"è·å– LangSmith traces å¤±è´¥ï¼š{exc}")
                # ä¸ä¸­æ–­è¯·æ±‚ï¼Œç»§ç»­è¿”å›æ¶ˆæ¯å†å²
        
        # æ„å»º trace æ ‘
        trace_tree = build_trace_tree(trace_runs)
        
        # æå–æ ¹èŠ‚ç‚¹ä¿¡æ¯ï¼ˆç”¨äºå‰ç«¯æ˜¾ç¤ºç»Ÿè®¡æ•°æ®ï¼‰
        root_run_id = None
        total_latency_ms = None
        total_tokens = None
        
        if trace_tree and len(trace_tree) > 0:
            root_run = trace_tree[0]
            root_run_id = root_run.get("run_id")
            total_latency_ms = root_run.get("latency_ms")
            
            # æ±‡æ€»æ‰€æœ‰ tokenï¼ˆé€’å½’ç»Ÿè®¡æ‰€æœ‰å­èŠ‚ç‚¹ï¼‰
            def sum_tokens(node: dict) -> int:
                # è·å–å½“å‰èŠ‚ç‚¹çš„ token
                tokens = 0
                if node.get("total_tokens"):
                    tokens = node["total_tokens"]
                elif node.get("prompt_tokens") or node.get("completion_tokens"):
                    # å¦‚æœæ²¡æœ‰ total_tokensï¼Œæ‰‹åŠ¨è®¡ç®—
                    tokens = (node.get("prompt_tokens") or 0) + (node.get("completion_tokens") or 0)
                
                # é€’å½’ç´¯åŠ æ‰€æœ‰å­èŠ‚ç‚¹çš„ token
                for child in node.get("children", []):
                    tokens += sum_tokens(child)
                
                return tokens
            
            total_tokens = sum_tokens(root_run)
            
            logger.info(
                f"ğŸ“Š Thread {thread_id} stats: "
                f"root_run_id={root_run_id}, "
                f"latency={total_latency_ms}ms, "
                f"tokens={total_tokens}"
            )
        
        return ThreadHistoryWithTrace(
            thread_id=thread_id,
            messages=history_messages,
            total_messages=len(history_messages),
            trace_runs=trace_runs,
            trace_tree=trace_tree,
            root_run_id=root_run_id,
            total_latency_ms=total_latency_ms,
            total_tokens=total_tokens,
        )

    except HTTPException:
        raise
    except Exception as exc:
        logger.exception(f"Error fetching thread history with trace: {exc}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to fetch thread history: {str(exc)}",
        ) from exc


@router.delete("/threads/{thread_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_thread(thread_id: str):
    """åˆ é™¤æŒ‡å®šçº¿ç¨‹çš„æ‰€æœ‰ checkpoint è®°å½•ã€‚

    âœ… æµç¨‹ï¼š
    1. è·å– checkpointer å®ä¾‹
    2. è°ƒç”¨ checkpointer.delete_thread() åˆ é™¤æ‰€æœ‰ checkpoint
    3. è¿”å› 204 No Content

    å‚æ•°ï¼š
    - thread_id: è¦åˆ é™¤çš„ä¼šè¯çº¿ç¨‹æ ‡è¯†

    è¿”å›ï¼š
    - 204 No Content: åˆ é™¤æˆåŠŸ

    æ³¨æ„ï¼š
    - åˆ é™¤æ˜¯æ°¸ä¹…çš„ï¼Œæ— æ³•æ¢å¤
    - ä¼šåˆ é™¤è¯¥ thread_id çš„æ‰€æœ‰ checkpoint è®°å½•
    """
    try:
        # æ ¹æ®å®é™…çš„ LangGraph checkpoint è¡¨ç»“æ„åˆ é™¤ï¼ˆ4 å¼ è¡¨ï¼‰ï¼š
        # - checkpoint_writes: å­˜å‚¨ checkpoint å†™å…¥æ•°æ®ï¼ˆä¾èµ– checkpoint_idï¼‰
        # - checkpoint_blobs: å­˜å‚¨ checkpoint äºŒè¿›åˆ¶æ•°æ®
        # - checkpoints: å­˜å‚¨ checkpoint å…ƒæ•°æ®ï¼ˆä¸»è¡¨ï¼‰
        # - checkpoint_migrations: è¿ç§»ç‰ˆæœ¬è¡¨ï¼ˆä¸éœ€è¦åˆ é™¤ï¼‰
        from db.database import DatabaseManager
        
        pool = await DatabaseManager.get_pool()
        async with pool.connection() as conn:
            async with conn.cursor() as cur:
                # æŒ‰ä¾èµ–é¡ºåºåˆ é™¤ï¼šå…ˆåˆ é™¤ä¾èµ–è¡¨ï¼Œå†åˆ é™¤ä¸»è¡¨
                # 1. åˆ é™¤ checkpoint_writesï¼ˆä¾èµ– checkpoint_idï¼‰
                await cur.execute(
                    "DELETE FROM checkpoint_writes WHERE thread_id = %s",
                    (thread_id,),
                )
                writes_deleted = cur.rowcount
                
                # 2. åˆ é™¤ checkpoint_blobs
                await cur.execute(
                    "DELETE FROM checkpoint_blobs WHERE thread_id = %s",
                    (thread_id,),
                )
                blobs_deleted = cur.rowcount
                
                # 3. åˆ é™¤ checkpointsï¼ˆä¸»è¡¨ï¼‰
                await cur.execute(
                    "DELETE FROM checkpoints WHERE thread_id = %s",
                    (thread_id,),
                )
                checkpoints_deleted = cur.rowcount
                
                await conn.commit()
                
                logger.info(
                    f"Successfully deleted {checkpoints_deleted} checkpoints, "
                    f"{writes_deleted} checkpoint_writes, "
                    f"and {blobs_deleted} checkpoint_blobs for thread {thread_id}"
                )
        
        return None
    except Exception as exc:
        logger.exception(f"Failed to delete thread {thread_id}: {exc}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"Failed to delete thread: {str(exc)}",
        )


__all__ = ["router"]

